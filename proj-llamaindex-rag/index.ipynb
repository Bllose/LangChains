{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 需要下载两个模型\n",
    "- Embedding Model\n",
    "- LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bllos\\miniconda3\\envs\\llamaindex-rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model Download\n",
    "model_id = 'Ceceliachenen/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "cache_dir = 'D:\\\\temp\\\\models'\n",
    "\n",
    "model_dir = snapshot_download(model_id=model_id, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading [model-00004-of-00004.safetensors]: 100%|██████████| 3.31G/3.31G [05:41<00:00, 10.4MB/s]\n",
      "Downloading [model.safetensors.index.json]: 100%|██████████| 27.1k/27.1k [00:00<00:00, 46.1kB/s]\n",
      "Downloading [README.md]: 100%|██████████| 6.09k/6.09k [00:00<00:00, 11.1kB/s]\n",
      "Downloading [tokenizer.json]: 100%|██████████| 6.71M/6.71M [00:01<00:00, 5.10MB/s]\n",
      "Downloading [tokenizer_config.json]: 100%|██████████| 7.13k/7.13k [00:00<00:00, 14.4kB/s]\n",
      "Downloading [vocab.json]: 100%|██████████| 2.65M/2.65M [00:00<00:00, 3.32MB/s]\n"
     ]
    }
   ],
   "source": [
    "# LLM model download\n",
    "model_id = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "cache_dir = 'D:\\\\temp\\\\models'\n",
    "\n",
    "model_dir = snapshot_download(model_id=model_id, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用本地模拟进行推理测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bllos\\miniconda3\\envs\\llamaindex-rag\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bllos\\miniconda3\\envs\\llamaindex-rag\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bllos\\miniconda3\\envs\\llamaindex-rag\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bllos\\miniconda3\\envs\\llamaindex-rag\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bllos\\miniconda3\\envs\\llamaindex-rag\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.43s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: XTuner 是一款由阿里云开发的机器学习超参数调优工具。它旨在帮助数据科学家和机器学习工程师更高效地找到最优模型配置，从而提高模型性能。XTuner 通过自动化的手段探索超参数空间，使用先进的算法来优化模型训练过程中的超参数设置，减少人工调参的时间和精力。\n",
      "\n",
      "XTuner 支持多种机器学习框架，如 PyTorch 和 TensorFlow 等，并且可以与阿里云的计算资源和服务无缝集成，提供灵活的资源配置选项，以满足不同规模任务的需求。通过使用 XTuner，用户能够专注于业务逻辑和模型设计，而将调参工作交给 XTuner 自动处理。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# 使用HuggingFace加载本地大模型\n",
    "llm = HuggingFaceLLM(\n",
    "    # 给定的时本地模型的全路径\n",
    "    model_name = 'D:\\\\temp\\\\models\\\\Qwen\\\\Qwen2___5-7B-Instruct',\n",
    "    tokenizer_name='D:\\\\temp\\\\models\\\\Qwen\\\\Qwen2___5-7B-Instruct',\n",
    "    model_kwargs={\"trust_remote_code\":True},\n",
    "    tokenizer_kwargs={\"trust_remote_code\":True}\n",
    ")\n",
    "\n",
    "rsp = llm.chat(messages=[ChatMessage(content=\"xtuner是什么?\")])\n",
    "print(rsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过RAG的方式，结合自己的知识，让AI回答问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name D:\\temp\\models\\Qwen\\Qwen2___5-7B-Instruct. Creating a new one with MEAN pooling.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:09<00:00, 32.35s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embed_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m embedding \u001b[38;5;241m=\u001b[39m HuggingFaceEmbedding(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mQwen\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mQwen2___5-7B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 将创建的嵌入模型赋值给全局的embed_model属性\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 这样在后续的索引构建过程中就会使用这个模型\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m Settings\u001b[38;5;241m.\u001b[39membed_model \u001b[38;5;241m=\u001b[39m \u001b[43membed_model\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 推理模型（生成模型）\u001b[39;00m\n\u001b[0;32m     10\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceLLM(\u001b[38;5;66;03m# 给定的时本地模型的全路径\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mQwen\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mQwen2___5-7B-Instruct\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     tokenizer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mQwen\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mQwen2___5-7B-Instruct\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[0;32m     14\u001b[0m     tokenizer_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embed_model' is not defined"
     ]
    }
   ],
   "source": [
    "# 初始化一个HuggingFaceEmbedding对象，用于将文本转换为向量表示\n",
    "# 指定了一个预训练的sentence-transformer模型的路径\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"D:\\\\temp\\\\models\\\\Ceceliachenen\\\\paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# 将创建的嵌入模型赋值给全局的embed_model属性\n",
    "# 这样在后续的索引构建过程中就会使用这个模型\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# 推理模型（生成模型）\n",
    "llm = HuggingFaceLLM(# 给定的时本地模型的全路径\n",
    "    model_name = 'D:\\\\temp\\\\models\\\\Qwen\\\\Qwen2___5-7B-Instruct',\n",
    "    tokenizer_name='D:\\\\temp\\\\models\\\\Qwen\\\\Qwen2___5-7B-Instruct',\n",
    "    model_kwargs={\"trust_remote_code\":True},\n",
    "    tokenizer_kwargs={\"trust_remote_code\":True}\n",
    ")\n",
    "\n",
    "# 设置全局的llm属性，这样在索引查询时会使用这个模型\n",
    "Settings.llm = llm\n",
    "\n",
    "# RAG 系统构建过程\n",
    "# 从指定目录读取所有文档，并加载数据到内存中，required_exts参数指定了只读取.md文件\n",
    "documents = SimpleDirectoryReader('D:\\\\temp\\\\data\\\\', required_exts=['.md']).load_data() \n",
    "\n",
    "# 创建一个VectorStoreIndex对象，并使用之前加载的文档来构建索引\n",
    "# 此索引将文档转换为向量，并存储这些向量以便于快速检索\n",
    "# 默认是存储在内存中的\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 创建一个查询引擎，这个引擎可以接收查询并返回相关文档的响应\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"陈曦是谁?\")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
